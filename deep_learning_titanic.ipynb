{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Stasiek/Developer/Anaconda/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Data cleanup\n",
    "# TRAIN DATA\n",
    "train_df = pd.read_csv('data/train.csv', header=0)        # Load the train file into a dataframe\n",
    "\n",
    "# I need to convert all strings to integer classifiers.\n",
    "# I need to fill in the missing values of the data and make it complete.\n",
    "\n",
    "# female = 0, Male = 1\n",
    "train_df['Gender'] = train_df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "# Embarked from 'C', 'Q', 'S'\n",
    "# Note this is not ideal: in translating categories to numbers, Port \"2\" is not 2 times greater than Port \"1\", etc.\n",
    "\n",
    "# All missing Embarked -> just make them embark from most common place\n",
    "if len(train_df.Embarked[ train_df.Embarked.isnull() ]) > 0:\n",
    "    train_df.Embarked[ train_df.Embarked.isnull() ] = train_df.Embarked.dropna().mode().values\n",
    "\n",
    "Ports = list(enumerate(np.unique(train_df['Embarked'])))    # determine all values of Embarked,\n",
    "Ports_dict = { name : i for i, name in Ports }              # set up a dictionary in the form  Ports : index\n",
    "train_df.Embarked = train_df.Embarked.map( lambda x: Ports_dict[x]).astype(int)     # Convert all Embark strings to int\n",
    "\n",
    "# All the ages with no data -> make the median of all Ages\n",
    "median_age = train_df['Age'].dropna().median()\n",
    "if len(train_df.Age[ train_df.Age.isnull() ]) > 0:\n",
    "    train_df.loc[ (train_df.Age.isnull()), 'Age'] = median_age\n",
    "\n",
    "# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)\n",
    "train_df = train_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1)\n",
    "\n",
    "# print train_df.head()\n",
    "\n",
    "labels = train_df['Survived']\n",
    "train_df = train_df.drop('Survived',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataframe):\n",
    "    std = dataframe.std(0)\n",
    "    mean = dataframe.mean(0)\n",
    "    return (dataframe - mean ) / std\n",
    "\n",
    "train_df = normalize_data(train_df)\n",
    "# print normalize_data(train_df).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "input_size = 7\n",
    "num_labels = 2\n",
    "labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "print labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Divide into training set and validation set\"\"\"\n",
    "train_size = 600\n",
    "\n",
    "train_df = train_df.values.astype(np.float32)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_df, labels = randomize(train_df, labels)\n",
    "\n",
    "valid_dataset = train_df[train_size::]\n",
    "train_dataset = train_df[0:train_size]\n",
    "\n",
    "valid_labels = labels[train_size::]\n",
    "train_labels = labels[0:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.56522787  2.7372694  -0.47427881  0.76719898  0.59917361 -1.94121289\n   0.73728102]\n [-1.56522787 -0.10457867 -0.47427881 -0.47340772 -0.12484967  0.58562523\n   0.73728102]\n [-0.36915749  1.20113528 -0.47427881 -0.47340772 -0.37639198  0.58562523\n  -1.35481262]\n [ 0.82691282 -0.64222562 -0.47427881 -0.47340772 -0.49411377  0.58562523\n  -1.35481262]\n [-1.56522787  0.12584145 -0.47427881 -0.47340772  0.36808875  0.58562523\n   0.73728102]]\n[[ 1.  0.]\n [ 1.  0.]\n [ 0.  1.]\n [ 0.  1.]\n [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# print train_dataset.shape, train_labels.shape\n",
    "# print valid_dataset.shape, valid_labels.shape\n",
    "\n",
    "print train_dataset[0:5]\n",
    "print train_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\nfloat32\n"
     ]
    }
   ],
   "source": [
    "print train_dataset.dtype\n",
    "print train_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA\n",
    "test_df = pd.read_csv('data/test.csv', header=0)        # Load the test file into a dataframe\n",
    "\n",
    "# I need to do the same with the test data now, so that the columns are the same as the training data\n",
    "# I need to convert all strings to integer classifiers:\n",
    "# female = 0, Male = 1\n",
    "test_df['Gender'] = test_df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "# Embarked from 'C', 'Q', 'S'\n",
    "# All missing Embarked -> just make them embark from most common place\n",
    "if len(test_df.Embarked[ test_df.Embarked.isnull() ]) > 0:\n",
    "    test_df.Embarked[ test_df.Embarked.isnull() ] = test_df.Embarked.dropna().mode().values\n",
    "# Again convert all Embarked strings to int\n",
    "test_df.Embarked = test_df.Embarked.map( lambda x: Ports_dict[x]).astype(int)\n",
    "\n",
    "\n",
    "# All the ages with no data -> make the median of all Ages\n",
    "median_age = test_df['Age'].dropna().median()\n",
    "if len(test_df.Age[ test_df.Age.isnull() ]) > 0:\n",
    "    test_df.loc[ (test_df.Age.isnull()), 'Age'] = median_age\n",
    "\n",
    "# All the missing Fares -> assume median of their respective class\n",
    "if len(test_df.Fare[ test_df.Fare.isnull() ]) > 0:\n",
    "    median_fare = np.zeros(3)\n",
    "    for f in range(0,3):                                              # loop 0 to 2\n",
    "        median_fare[f] = test_df[ test_df.Pclass == f+1 ]['Fare'].dropna().median()\n",
    "    for f in range(0,3):                                              # loop 0 to 2\n",
    "        test_df.loc[ (test_df.Fare.isnull()) & (test_df.Pclass == f+1 ), 'Fare'] = median_fare[f]\n",
    "\n",
    "# Collect the test data's PassengerIds before dropping it\n",
    "ids = test_df['PassengerId'].values\n",
    "# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)\n",
    "test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) \n",
    "\n",
    "\n",
    "# The data is now ready to go. So lets fit to the train, then predict to the test!\n",
    "# Convert back to a numpy array\n",
    "test_data = test_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph() \n",
    "num_hidden = 20\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    # Load the training, validation data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_data)\n",
    "    \n",
    "    # Variables\n",
    "    # Parameters which are going to be trained.\n",
    "    weights = tf.Variable(tf.truncated_normal([input_size, num_hidden]))\n",
    "    biases = tf.Variable(tf.zeros([num_hidden]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    hiddenLayer = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    secondLayer = tf.matmul(hiddenLayer, weights2) + biases2\n",
    "  \n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(secondLayer, tf_train_labels))\n",
    "  \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for training and validation sets.\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(secondLayer)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), weights2) + biases2 )\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), weights2) + biases2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nLoss at step 0: 3.824332\nTraining accuracy: 38.7%\nValidation accuracy: 68.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 100: 0.403742\nTraining accuracy: 82.2%\nValidation accuracy: 81.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 200: 0.375527\nTraining accuracy: 83.2%\nValidation accuracy: 79.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 300: 0.361495\nTraining accuracy: 84.3%\nValidation accuracy: 78.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400: 0.352226\nTraining accuracy: 85.2%\nValidation accuracy: 79.7%\nPredicting...\nDone.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 401\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time coperation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, train_labels))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "    print 'Predicting...'\n",
    "    output = np.argmax(test_prediction.eval(),1)\n",
    "    \n",
    "    predictions_file = open(\"deeplearning.csv\", \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"PassengerId\",\"Survived\"])\n",
    "    open_file_object.writerows(zip(ids, output))\n",
    "    predictions_file.close()\n",
    "    print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "regularization = 0.001\n",
    "hiddenLayer1 = 15\n",
    "hiddenLayer2 = 15\n",
    "\n",
    "graph = tf.Graph() \n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    # Load the training, validation data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, input_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_data)\n",
    "    \n",
    "    # Variables\n",
    "    # Parameters which are going to be trained.\n",
    "  \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([input_size, hiddenLayer1]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([hiddenLayer1]))\n",
    "    layer2_weigths = tf.Variable(tf.truncated_normal([hiddenLayer1, hiddenLayer2]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([hiddenLayer2]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([hiddenLayer2, num_labels]))\n",
    "    layer3_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Model\n",
    "    def model(data,training = False):\n",
    "        # Computing first layer\n",
    "        layer1 = tf.nn.relu(tf.matmul(data, layer1_weights) + layer1_biases)\n",
    "        if training:\n",
    "            layer1 = tf.nn.dropout(layer1, 0.75)\n",
    "        # Computing second layer\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, layer2_weigths) + layer2_biases)\n",
    "        if training:\n",
    "            layer2 = tf.nn.dropout(layer2, 0.75)\n",
    "        \n",
    "        return tf.matmul(layer2, layer3_weights) + layer3_biases\n",
    "  \n",
    "    # Training computations\n",
    "    logits = model(tf_train_dataset,True)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + regularization * ( tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weigths) + tf.nn.l2_loss(layer3_weights))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for training and validation sets.\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nMinibatch loss at step 0: 9.081197\nMinibatch accuracy: 40.0%\nValidation accuracy: 65.6%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 50: 0.549266\nMinibatch accuracy: 88.0%\nValidation accuracy: 79.7%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 100: 0.748935\nMinibatch accuracy: 64.0%\nValidation accuracy: 78.7%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 150: 0.652652\nMinibatch accuracy: 72.0%\nValidation accuracy: 78.7%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 200: 0.573830\nMinibatch accuracy: 80.0%\nValidation accuracy: 82.8%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 250: 0.462683\nMinibatch accuracy: 92.0%\nValidation accuracy: 79.7%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 300: 0.633374\nMinibatch accuracy: 68.0%\nValidation accuracy: 82.5%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 350: 0.556082\nMinibatch accuracy: 76.0%\nValidation accuracy: 79.0%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 400: 0.477148\nMinibatch accuracy: 84.0%\nValidation accuracy: 81.4%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 450: 0.440259\nMinibatch accuracy: 88.0%\nValidation accuracy: 80.8%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 0.351321\nMinibatch accuracy: 92.0%\nValidation accuracy: 82.5%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 550: 0.459708\nMinibatch accuracy: 76.0%\nValidation accuracy: 81.1%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 600: 0.695222\nMinibatch accuracy: 72.0%\nValidation accuracy: 80.8%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 650: 0.436098\nMinibatch accuracy: 84.0%\nValidation accuracy: 81.8%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 700: 0.375798\nMinibatch accuracy: 84.0%\nValidation accuracy: 82.8%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 750: 0.789337\nMinibatch accuracy: 68.0%\nValidation accuracy: 81.1%\nPredicting...\nMinibatch loss at step 800: 0.757821\nMinibatch accuracy: 64.0%\nValidation accuracy: 81.4%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 850: 0.585755\nMinibatch accuracy: 84.0%\nValidation accuracy: 82.1%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 900: 0.484738\nMinibatch accuracy: 84.0%\nValidation accuracy: 79.7%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 950: 0.685635\nMinibatch accuracy: 76.0%\nValidation accuracy: 82.1%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 0.551824\nMinibatch accuracy: 76.0%\nValidation accuracy: 80.1%\nPredicting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1050: 0.526847\nMinibatch accuracy: 80.0%\nValidation accuracy: 81.1%\nPredicting...\nDone.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1051\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            # print('Learning rate at step %d %f' % (step, learning_rate.eval() ))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "            print 'Predicting...'\n",
    "    \n",
    "    output = np.argmax(test_prediction.eval(),1)\n",
    "    \n",
    "    predictions_file = open(\"doubleHiddenLayer.csv\", \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"PassengerId\",\"Survived\"])\n",
    "    open_file_object.writerows(zip(ids, output))\n",
    "    predictions_file.close()\n",
    "    print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}